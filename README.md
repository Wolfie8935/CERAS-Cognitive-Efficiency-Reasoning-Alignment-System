# CERAS â€” Cognitive Efficiency & Reasoning Alignment System

CERAS is a research and prototype framework that measures **cognitive learning readiness** using hybrid AI systems such as **ANFIS**, **ANN**, and **Genetic Algorithms**.

## Setup
```bash
conda env create -f environment.yml
conda activate ceras
```


## Overview

**CERAS (Cognitive Efficiency & Reasoning Alignment System)** is a hybrid intelligent framework that models and measures an individualâ€™s **learning readiness** by combining **Cognitive Efficiency (CE)** and **Reasoning Alignment (RA)**.

It unifies:
- **Neuro-Fuzzy Systems (ANFIS)** for interpretable rule-based reasoning,  
- **Neural Networks (ANN)** for adaptive pattern discovery,  
- **Genetic Algorithms (GA)** for optimal parameter and rule search,  
- **Large Language Models (LLMs)** via **LangChain + Ollama** for reasoning decomposition and verification.

The system evaluates **how efficiently** a learner processes information and **how closely** their reasoning aligns with domain experts â€” producing an interpretable, multi-dimensional *Learning Readiness Score (LRS)*.

---

## Objectives

1. **Measure Cognitive Efficiency (CE):** Quantify the learnerâ€™s speed, focus, and information-processing efficiency from behavioral signals.  
2. **Evaluate Reasoning Alignment (RA):** Compare the learnerâ€™s logical structure with expert reasoning trees using fuzzy-semantic scoring.  
3. **Decompose & Verify Reasoning:** Use an LLM-based *Reasoning Decomposer* and *Verifier* to generate and validate atomic subproblems from user queries.  
4. **Compute Learning Readiness Score (LRS):** Fuse CE and RA via a fuzzy-weighted inference engine.  
5. **Enable Explainability:** Produce interpretable rules and alignment graphs explaining *why* a learner scored high or low.  
6. **Deliver a Full Interactive System:** Deploy a web dashboard and API for exploration, live reasoning, and model interaction.

---

## Tech Stack Summary

| Layer | Tools / Frameworks | Purpose |
|-------|--------------------|----------|
| Language | **Python 3.12** | Core implementation |
| Orchestration | **LangChain (0.3.27)** + **Ollama** | Reasoning decomposition and verification |
| ML Frameworks | **PyTorch**, **Scikit-learn** | Neural & hybrid learning |
| Neuro-Fuzzy | **Custom ANFIS** | Interpretable fuzzy inference |
| Optimization | **Genetic Algorithm (GA)** | Membership function optimization |
| Vector Search | **FAISS**, **ChromaDB** | Reasoning memory and context retrieval |
| Interface | **Streamlit**, **FastAPI**, **Uvicorn** | Dashboard + REST API |
| Experimentation | **JupyterLab**, **Matplotlib**, **Seaborn** | Analysis & visualization |
| Logging & Config | **Loguru**, **PyYAML** | Config + monitoring |
| Deployment | **Docker**, **GitHub Actions** | CI/CD + reproducibility |

---

## System Architecture

![System Architecture ceras](./docs/image.png)


## ðŸ§® Pipeline Flow (Stage-by-Stage)

| Stage | Module | Description | Output |
|--------|---------|--------------|----------|
| **1. Data Ingestion** | `src/ceras/data_loader.py` | Load raw behavioral, cognitive, and reasoning datasets from CSV/JSON sources. | Pandas DataFrame |
| **2. Preprocessing** | `src/ceras/features/featurize.py` | Clean, normalize, and engineer cognitive features (reaction time, accuracy, focus metrics). | Feature Matrix |
| **3. Feature Engineering & Quality Layer** | `src/ceras/features/feature_quality.py` | Apply temporal smoothing, embeddings, missingness modeling, and outlier detection. | Cleaned, aligned data tensors |
| **4. Cognitive Efficiency Modeling (CEPM)** | `src/ceras/models/cepm.py` | Sequence encoder (GRU) + XGBoost head for CE score prediction; explainable with SHAP. | **CE Score (0â€“100)** |
| **5. Reasoning Trace Segmentation** | `src/ceras/reasoning/trace_segmenter.py` | Segment reasoning logs or text traces into discrete cognitive steps. | Stepwise trace dataset |
| **6. Reasoning Decomposition** | `src/ceras/reasoning/decomposer.py` | LLM (`llm_utils.py`) breaks input problem into **atomic subtasks** via strict JSON: `{"subtasks": [...]}` | Subtask JSON |
| **7. Subtask Verification Loop** | `src/ceras/reasoning/verifier.py` | Separate LLM verifier evaluates subtask quality.<br>â€¢ If `{approved: true}` â†’ forward to CAMRE<br>â€¢ If `{approved: false}` â†’ re-decompose using feedback | Verified Subtasks |
| **8. CAMRE-EDU Reasoning Analysis** | `src/ceras/reasoning/camre_edu.py` | Builds reasoning tree-of-thoughts graph; aligns user reasoning with expert reference. | **RDS (0.0â€“1.0)** + Breakpoints |
| **9. Inference Engine** | `src/ceras/inference/engine.py` | Combines CE and RDS â†’ fuzzy aggregation â†’ Learning Readiness Score (LRS). | **LRS (0.0â€“1.0)** + Explanation Rules |
| **10. Interface Layer** | `src/ceras/interface/` (Streamlit / FastAPI) | Web UI for reasoning exploration, CE/RDS dashboards, and API endpoints. | Interactive Visualization |
| **11. Experiment Tracking & Reporting** | `experiments/run_experiment.py` | Logs model runs, visualizes CEâ€“RDS correlation, generates analysis reports. | Logs, graphs, and summaries |

---

## ðŸ” Reasoning Engine Loop

The Reasoning Engine has an internal **verification-feedback loop** for decomposition quality control.

```mermaid
flowchart TD
    Q["User Query / Reasoning Trace"] --> D["Decomposer (LLM)\nâ†’ Strict JSON: {subtasks:[...]}"]
    D --> V["Verifier (LLM Classifier)\nâ†’ {approved: bool, confidence, revised_subtasks?}"]

    V -- "approved: true" --> A["CAMRE-EDU\n(Tree-of-Thoughts Graph Alignment)"]
    V -- "approved: false" --> D2["Re-Decomposer (Refine using verifier feedback)"]
    D2 --> V

    A --> RDS["RDS (0â€“1) + Breakpoints"]
```

**Key Points:**
- Decomposer: always returns JSON; failure raises exception (no fallback).  
- Verifier: ensures correctness and logical order of subtasks.  
- If the verifier rejects (`approved: false`), decomposition re-runs with structured feedback.  
- Loop terminates when `approved: true` or max attempts reached.

---

## ðŸ”€ Integration with CEPM

After reasoning validation and CAMRE scoring:

```mermaid
flowchart LR
    CE["CEPM (Cognitive Efficiency Model)"] --> INFER["CERAS Inference Engine"]
    RDS["CAMRE-EDU RDS"] --> INFER
    INFER --> LRS["Learning Readiness Score (LRS)\n= f(CE, RDS)"]
    LRS --> DASH["Streamlit / API Dashboard\nâ†’ Reports, Readiness Graphs, CEÃ—RDS Analytics"]
```

**Learning Readiness Score (LRS)** is computed as:

\[
LRS = \alpha \times f(CE) + (1-\alpha) \times g(RDS)
\]

Where:
- `f()` and `g()` are fuzzy membership functions learned via ANFIS.  
- `Î±` is optimized using GA to balance efficiency vs reasoning.

---

## ðŸ“Š Key Outputs

| Metric | Description |
|---------|--------------|
| **CE (Cognitive Efficiency)** | Behavioral efficiency (0â€“100) |
| **RDS (Reasoning Divergence Score)** | Logical alignment with expert graph (0â€“1) |
| **LRS (Learning Readiness Score)** | Combined readiness indicator |
| **Breakpoints** | Reasoning divergence points |
| **Rule Confidence** | Fuzzy rule trust coefficient |
| **Interpretability Index** | Fraction of explainable rules |

---

## ðŸ§  Example Inference Output

```json
{
  "query": "How to develop an app in Flutter?",
  "subtasks": [
    "Define app structure and target features",
    "Set up Flutter environment and SDK",
    "Design UI components and navigation flow",
    "Connect backend APIs and manage state",
    "Test and debug on multiple devices",
    "Build release version and deploy to store"
  ],
  "verifier": {"approved": true, "confidence": 0.96},
  "RDS": 0.84,
  "CE": 79.3,
  "LRS": 0.87
}
```

---

## ðŸ§© File Mapping (Core Modules)

```
ceras/
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ preprocess/                      
â”‚   â”œâ”€â”€ __init__.py    
â”‚   â”œâ”€â”€ signal_fusion.py            
â”‚   â””â”€â”€ auditor.py                  
â”‚       
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ featurize.py
â”‚   â””â”€â”€ feature_quality.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cepm.py
â”‚   â”œâ”€â”€ anfis.py
â”‚   â””â”€â”€ ann.py
â”‚
â”œâ”€â”€ postprocess/                     
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ calibrator.py               
â”‚   â”œâ”€â”€ insights.py               
â”‚   â””â”€â”€ model_monitor.py      
â”‚
â”œâ”€â”€ infra/                          
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiment_tracking.py       
â”‚   â””â”€â”€ llm_queue.py                 
â”‚
â”œâ”€â”€ reasoning/
â”‚   â”œâ”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ decomposer.py
â”‚   â”œâ”€â”€ verifier.py
â”‚   â””â”€â”€ camre_edu.py
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ engine.py
â”‚   â””â”€â”€ fusion.py                  
â”‚
â”œâ”€â”€ interface/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ fastapi_app.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ synthetic_benchmarks.py     
â”‚
â”œâ”€â”€ monitoring/                       
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ alerts.py                    
â”‚   â””â”€â”€ reports.py                  
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_preprocess.py           
    â”œâ”€â”€ test_postprocess.py        
    â””â”€â”€ test_synthetic_benchmarks.py
```

---
