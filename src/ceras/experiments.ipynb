{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f4a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtasks:\n",
      " 1. Find online resources for learning French.\n",
      " 2. Identify language learning apps suitable for beginners.\n",
      " 3. Enroll in a French language course or find a tutor.\n",
      " 4. Watch French TV shows and movies with English subtitles.\n",
      " 5. Practice speaking with a native speaker or language exchange partner.\n",
      " 6. Learn basic French phrases and vocabulary using flashcards or a dictionary.\n",
      " 7. Immerse yourself in the French language by listening to French music and podcasts.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Find online resources for learning French.', 'Identify language learning apps suitable for beginners.', 'Enroll in a French language course or find a tutor.', 'Watch French TV shows and movies with English subtitles.', 'Practice speaking with a native speaker or language exchange partner.', 'Learn\n",
      "Warning: verifier did not return clean JSON. Falling back to heuristic coverage. Error: substring not found\n",
      "Subtasks:\n",
      " 1. Open a search engine website (e.g., Google, Bing).\n",
      " 2. Enter keywords \"learn French\" or similar phrases into the search bar.\n",
      " 3. Filter results by categories such as \"online courses,\" \"tutorials,\" or \"resources.\"\n",
      " 4. Select one or more relevant online resources to explore further.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Open a search engine website (e.g., Google, Bing).', 'Enter keywords \"learn French\" or similar phrases into the search bar.', 'Filter results by categories such as \"online courses,\" \"tutorials,\" or \"resources.\"', 'Select one or more relevant online resources to explore further.']\n",
      "Subtasks:\n",
      " 1. Look up popular language learning apps.\n",
      " 2. Filter results by beginner-friendly features.\n",
      " 3. Check app reviews for user feedback on effectiveness.\n",
      " 4. Compare features, pricing, and ratings of shortlisted apps.\n",
      " 5. Select top recommendations for further research or testing.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Look up popular language learning apps.', 'Filter results by beginner-friendly features.', 'Check app reviews for user feedback on effectiveness.', 'Compare features, pricing, and ratings of shortlisted apps.', 'Select top recommendations for further research or testing.']\n",
      "Subtasks:\n",
      " 1. Research French language courses offered locally.\n",
      " 2. Look for online French language courses or tutorials.\n",
      " 3. Find a local tutor who offers one-on-one instruction in French.\n",
      " 4. Check reviews and ratings of potential tutors before selecting one.\n",
      " 5. Schedule a trial lesson or consultation with the chosen tutor.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research French language courses offered locally.', 'Look for online French language courses or tutorials.', 'Find a local tutor who offers one-on-one instruction in French.', 'Check reviews and ratings of potential tutors before selecting one.', 'Schedule a trial lesson or consultation with the c\n",
      "Subtasks:\n",
      " 1. Open Netflix or Amazon Prime Video.\n",
      " 2. Search for \"French TV shows\" on the platform.\n",
      " 3. Select a French TV show to watch.\n",
      " 4. Enable English subtitles on the selected show.\n",
      " 5. Search for \"French movies\" on the platform.\n",
      " 6. Select a French movie to watch.\n",
      " 7. Enable English subtitles on the selected movie.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Open Netflix or Amazon Prime Video.', 'Search for \"French TV shows\" on the platform.', 'Select a French TV show to watch.', 'Enable English subtitles on the selected show.', 'Search for \"French movies\" on the platform.', 'Select a French movie to watch.', 'Enable English subtitles on the selected \n",
      "Subtasks:\n",
      " 1. Search for online platforms offering language exchange services.\n",
      " 2. Create an account on one of these platforms.\n",
      " 3. Browse profiles to find suitable language partners (native speakers).\n",
      " 4. Initiate contact with selected partners via messaging system.\n",
      " 5. Discuss and agree upon meeting schedule, topic preferences, or practice formats.\n",
      " 6. Schedule a video call or in-person meeting to engage in conversation.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Search for online platforms offering language exchange services.', 'Create an account on one of these platforms.', 'Browse profiles to find suitable language partners (native speakers).', 'Initiate contact with selected partners via messaging system.', 'Discuss and agree upon meeting schedule, top\n",
      "Subtasks:\n",
      " 1. Find reliable online resources offering interactive French language learning materials.\n",
      " 2. Create physical flashcard sets with new vocabulary words and translations.\n",
      " 3. Consult and use a comprehensive French-English dictionary to expand vocabulary knowledge.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Find reliable online resources offering interactive French language learning materials.', 'Create physical flashcard sets with new vocabulary words and translations.', 'Consult and use a comprehensive French-English dictionary to expand vocabulary knowledge.']\n",
      "Subtasks:\n",
      " 1. Find playlists or radio stations that play French music.\n",
      " 2. Discover popular French-language podcasts on platforms such as Spotify, Apple Podcasts, or Google Podcasts.\n",
      " 3. Create a schedule to dedicate time each day or week to listening to the chosen content.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Find playlists or radio stations that play French music.', 'Discover popular French-language podcasts on platforms such as Spotify, Apple Podcasts, or Google Podcasts.', 'Create a schedule to dedicate time each day or week to listening to the chosen content.']\n",
      "Subtasks:\n",
      " 1. Research \"French sentence structure\" online.\n",
      " 2. Purchase a French grammar book or online course.\n",
      " 3. Spend 15-30 minutes per day studying the book/course for two weeks.\n",
      " 4. Practice speaking and writing in French to apply new knowledge.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research \"French sentence structure\" online.', 'Purchase a French grammar book or online course.', 'Spend 15-30 minutes per day studying the book/course for two weeks.', 'Practice speaking and writing in French to apply new knowledge.']\n",
      "Subtasks:\n",
      " 1. Check if there are any accepted items in the database.\n",
      " 2. If yes, retrieve their details (id, title, description, status).\n",
      " 3. Sort the results by the most recent update date or creation date in descending order.\n",
      " 4. Display the results to the user.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Check if there are any accepted items in the database.', 'If yes, retrieve their details (id, title, description, status).', 'Sort the results by the most recent update date or creation date in descending order.', 'Display the results to the user.']\n",
      "Subtasks:\n",
      " 1. Search for \"forwarding inference\" in machine learning or artificial intelligence literature.\n",
      " 2. Identify subtasks related to forwarding inference.\n",
      " 3. List potential subtasks of forwarding inference.\n",
      " 4. Select relevant and actionable subtasks from the list.\n",
      " 5. Provide an explanation of each selected subtask.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Search for \"forwarding inference\" in machine learning or artificial intelligence literature.', 'Identify subtasks related to forwarding inference.', 'List potential subtasks of forwarding inference.', 'Select relevant and actionable subtasks from the list.', 'Provide an explanation of each selecte\n",
      "Subtasks:\n",
      " 1. Research online for recommended grammar books on French.\n",
      " 2. Look up courses that focus on French sentence structure on online learning platforms (e.g., Coursera, Udemy).\n",
      " 3. Check out existing French language learning resources and textbooks with a strong grammar focus.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research online for recommended grammar books on French.', 'Look up courses that focus on French sentence structure on online learning platforms (e.g., Coursera, Udemy).', 'Check out existing French language learning resources and textbooks with a strong grammar focus.']\n",
      "\n",
      "Top nodes by combined_score:\n",
      "- id=ddc3ee8c score=0.0 role=final_subtask text=Find playlists or radio stations that play French music.\n",
      "- id=e62b60b1 score=0.0 role=final_subtask text=Create physical flashcard sets with new vocabulary words and translations.\n",
      "- id=ec9cf222 score=0.0 role=final_subtask text=Look up popular language learning apps.\n",
      "- id=f1b107ba score=0.0 role=final_subtask text=Research \"French sentence structure\" online.\n",
      "- id=ea52519a score=0.0 role=final_subtask text=Identify subtasks related to forwarding inference.\n",
      "\n",
      "Best path (by summed combined_score): 0.0\n",
      " * [root] how can i learn french? (score=None)\n",
      " * [final_subtask] Find online resources for learning French. (score=None)\n",
      " * [final_subtask] Open a search engine website (e.g., Google, Bing). (score=0.0)\n",
      "\n",
      "Saved tree to tree_of_thoughts_example.json\n",
      "\n",
      "=== FORMATTED FINAL SUBTASKS ===\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "4. \n",
      "5. \n",
      "6. \n",
      "7. \n",
      "\n",
      "=== REASONING DIAGNOSTIC ===\n",
      "{\n",
      "  \"reasoning_score\": 0.8365520432327374,\n",
      "  \"components\": {\n",
      "    \"goal_alignment\": 0.846442848443985,\n",
      "    \"stepwise_coherence\": 0.7562019278605779,\n",
      "    \"coverage\": 1.0,\n",
      "    \"granularity\": 0.4356870673439741,\n",
      "    \"redundancy\": 1.0\n",
      "  },\n",
      "  \"diagnostics\": {\n",
      "    \"stepwise_similarities\": [\n",
      "      0.7325788736343384,\n",
      "      0.7259098440408707,\n",
      "      0.8395350575447083,\n",
      "      0.7218140214681625,\n",
      "      0.699702650308609,\n",
      "      0.8176711201667786\n",
      "    ],\n",
      "    \"missing_concepts\": [],\n",
      "    \"expected_concepts\": 7,\n",
      "    \"average_step_length\": 9.428571428571429,\n",
      "    \"unique_clusters\": 1.0,\n",
      "    \"num_steps\": 7,\n",
      "    \"verifier_notes\": \"fallback heuristic used\"\n",
      "  },\n",
      "  \"verdict\": \"Reasoning is coherent and well-aligned with goal.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "from tree_of_thoughts import TreeOfThoughts\n",
    "from inference import run_inference_pipeline\n",
    "from CAMRE_EDU import combined_reasoning_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --------------------\n",
    "# Helper conversion utilities (no try/except per user request)\n",
    "# --------------------\n",
    "def safe_get_text(item):\n",
    "    \"\"\"\n",
    "    Extract text from common shapes:\n",
    "    - if str -> return it\n",
    "    - if dict -> try common keys\n",
    "    - if list/tuple -> join subitems recursively\n",
    "    \"\"\"\n",
    "    if item is None:\n",
    "        return \"\"\n",
    "    if isinstance(item, str):\n",
    "        return item\n",
    "    if isinstance(item, (list, tuple)):\n",
    "        parts = [safe_get_text(x) for x in item]\n",
    "        return \" \".join([p for p in parts if p])\n",
    "    if isinstance(item, dict):\n",
    "        for k in (\"text\", \"input\", \"output\", \"content\", \"message\", \"prompt\"):\n",
    "            v = item.get(k)\n",
    "            if v:\n",
    "                return safe_get_text(v)\n",
    "        parts = []\n",
    "        for v in item.values():\n",
    "            if isinstance(v, (str, list, tuple, dict)):\n",
    "                parts.append(safe_get_text(v))\n",
    "        return \" \".join([p for p in parts if p]) or \"\"\n",
    "    return str(item)\n",
    "\n",
    "\n",
    "def ensure_scores(d):\n",
    "    \"\"\"Return a dict of numeric scores. If d is missing, return {}.\"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {k: v for k, v in d.items() if isinstance(v, (int, float))}\n",
    "\n",
    "\n",
    "def pipeline_to_candidates(pipe_out):\n",
    "    \"\"\"\n",
    "    Convert pipeline output (which may have strings or dicts) into a canonical list of candidate dicts:\n",
    "      {\"text\",\"role\",\"scores\",\"metadata\",\"embedding\"}\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    if not isinstance(pipe_out, dict):\n",
    "        if isinstance(pipe_out, str):\n",
    "            candidates.append({\"text\": pipe_out, \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "        elif isinstance(pipe_out, (list, tuple)):\n",
    "            for it in pipe_out:\n",
    "                candidates.append({\"text\": safe_get_text(it), \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": it}})\n",
    "        return candidates\n",
    "\n",
    "    def ingest_field(field_value, role_hint):\n",
    "        if field_value is None:\n",
    "            return\n",
    "        if isinstance(field_value, (list, tuple)):\n",
    "            for item in field_value:\n",
    "                text = safe_get_text(item)\n",
    "                meta = item if isinstance(item, dict) else {\"orig\": item}\n",
    "                scores = ensure_scores(item.get(\"scores\") if isinstance(item, dict) else {})\n",
    "                candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "        else:\n",
    "            text = safe_get_text(field_value)\n",
    "            meta = field_value if isinstance(field_value, dict) else {\"orig\": field_value}\n",
    "            scores = ensure_scores(field_value.get(\"scores\") if isinstance(field_value, dict) else {})\n",
    "            candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "\n",
    "    ingest_field(pipe_out.get(\"final_subtasks\"), \"final_subtask\")\n",
    "    ingest_field(pipe_out.get(\"suggestions\"), \"suggestion\")\n",
    "    ingest_field(pipe_out.get(\"outputs\"), \"output\")\n",
    "    ingest_field(pipe_out.get(\"thoughts\"), \"thought\")\n",
    "    ingest_field(pipe_out.get(\"intermediate_steps\"), \"thought\")\n",
    "\n",
    "    known_keys = {\"final_subtasks\", \"suggestions\", \"outputs\", \"thoughts\", \"intermediate_steps\"}\n",
    "    for k, v in pipe_out.items():\n",
    "        if k in known_keys:\n",
    "            continue\n",
    "        if isinstance(v, (str, list, tuple, dict)):\n",
    "            ingest_field(v, role_hint=f\"field:{k}\")\n",
    "\n",
    "    if not candidates:\n",
    "        fallback_text = safe_get_text(pipe_out.get(\"text\") or pipe_out.get(\"content\") or pipe_out)\n",
    "        if fallback_text:\n",
    "            candidates.append({\"text\": fallback_text, \"role\": \"fallback\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def expand_fn_using_pipeline(prompt_text, metadata=None):\n",
    "    \"\"\"\n",
    "    Expansion function: call run_inference_pipeline(prompt_text, auto_extend=False)\n",
    "    Return candidate list with attached combined_score where possible.\n",
    "    \"\"\"\n",
    "    out = run_inference_pipeline(prompt_text, auto_extend=False)\n",
    "    cands = pipeline_to_candidates(out)\n",
    "\n",
    "    reasoning = combined_reasoning_score(out, prompt_text)\n",
    "\n",
    "    for c in cands:\n",
    "        c.setdefault(\"scores\", {})\n",
    "        # Prefer explicit combined_score if present\n",
    "        if \"combined_score\" in c[\"scores\"] and isinstance(c[\"scores\"][\"combined_score\"], (int, float)):\n",
    "            pass\n",
    "        else:\n",
    "            # derive from reasoning if a numeric field exists\n",
    "            if isinstance(reasoning, dict):\n",
    "                for key in (\"combined\", \"confidence\", \"score\", \"overall\"):\n",
    "                    val = reasoning.get(key)\n",
    "                    if isinstance(val, (int, float)):\n",
    "                        c[\"scores\"][\"combined_score\"] = float(val)\n",
    "                        break\n",
    "        # fallback to metadata scores average\n",
    "        meta_scores = c.get(\"metadata\", {})\n",
    "        if \"combined_score\" not in c[\"scores\"]:\n",
    "            if isinstance(meta_scores, dict) and isinstance(meta_scores.get(\"scores\"), dict):\n",
    "                numeric_vals = [v for v in meta_scores[\"scores\"].values() if isinstance(v, (int, float))]\n",
    "                if numeric_vals:\n",
    "                    c[\"scores\"][\"combined_score\"] = float(sum(numeric_vals) / len(numeric_vals))\n",
    "        c[\"scores\"].setdefault(\"combined_score\", 0.0)\n",
    "        # attach reasoning snapshot to metadata\n",
    "        c.setdefault(\"metadata\", {})\n",
    "        c[\"metadata\"][\"reasoning_snapshot\"] = reasoning\n",
    "\n",
    "    return cands\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Main integration that builds the TreeOfThoughts\n",
    "# --------------------\n",
    "def main():\n",
    "    example_prompt = \"how can i learn french?\"\n",
    "\n",
    "    # Create tree and root\n",
    "    tree = TreeOfThoughts()\n",
    "    root = tree.add_node(text=example_prompt, parent_id=None, role=\"root\", metadata={\"source\": \"user_prompt\"})\n",
    "    tree.set_root(root.id)\n",
    "\n",
    "    # Run pipeline once with auto_extend True to get full initial output\n",
    "    out = run_inference_pipeline(example_prompt, auto_extend=True)\n",
    "\n",
    "    # Convert pipeline output into initial candidate nodes and attach as children\n",
    "    initial_candidates = pipeline_to_candidates(out)\n",
    "    for cand in initial_candidates:\n",
    "        tree.add_node(\n",
    "            text=cand[\"text\"],\n",
    "            parent_id=root.id,\n",
    "            role=cand.get(\"role\"),\n",
    "            scores=cand.get(\"scores\", {}),\n",
    "            metadata=cand.get(\"metadata\", {})\n",
    "        )\n",
    "\n",
    "    # Attach combined reasoning diagnostic to root metadata\n",
    "    reasoning_result = combined_reasoning_score(out, example_prompt)\n",
    "    tree.nodes[root.id].metadata[\"reasoning_diagnostic\"] = reasoning_result\n",
    "\n",
    "    # Also attach same diagnostic snapshot to top-level children metadata\n",
    "    for child_id in tree.nodes[root.id].children:\n",
    "        tree.nodes[child_id].metadata.setdefault(\"root_reasoning\", reasoning_result)\n",
    "\n",
    "    # Expand each top-level child once using the pipeline expansion function (beam size configurable)\n",
    "    beam_size = 2\n",
    "    for child_id in list(tree.nodes[root.id].children):\n",
    "        tree.expand_with(child_id, expand_fn=expand_fn_using_pipeline, beam=beam_size)\n",
    "\n",
    "    # Example: print top 5 nodes by combined_score\n",
    "    top_nodes = tree.top_k_by_score(\"combined_score\", k=5)\n",
    "    print(\"\\nTop nodes by combined_score:\")\n",
    "    for n in top_nodes:\n",
    "        print(f\"- id={n.id[:8]} score={n.scores.get('combined_score')} role={n.role} text={n.text[:200]}\")\n",
    "\n",
    "    # Example: compute best path by summed combined_score\n",
    "    path_nodes, path_score = tree.best_path(\"combined_score\", max_depth=6)\n",
    "    print(\"\\nBest path (by summed combined_score):\", path_score)\n",
    "    for n in path_nodes:\n",
    "        print(f\" * [{n.role}] {n.text[:200]} (score={n.scores.get('combined_score')})\")\n",
    "\n",
    "    # Save tree to disk\n",
    "    outpath = \"tree_of_thoughts_example.json\"\n",
    "    tree.save_json(outpath)\n",
    "    print(f\"\\nSaved tree to {outpath}\")\n",
    "\n",
    "    # Print formatted final subtasks (as before)\n",
    "    print(\"\\n=== FORMATTED FINAL SUBTASKS ===\")\n",
    "    for i, s in enumerate(out.get(\"final_subtasks\", []), start=1):\n",
    "        text = safe_get_text(s.get(\"input\") if isinstance(s, dict) else s) if isinstance(s, (dict, str)) else safe_get_text(s)\n",
    "        print(f\"{i}. {text}\")\n",
    "\n",
    "    print(\"\\n=== REASONING DIAGNOSTIC ===\")\n",
    "    print(json.dumps(reasoning_result, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e9356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1cb95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtasks:\n",
      " 1. Research and find reliable French language learning resources online.\n",
      " 2. Determine the level of proficiency desired (beginner, intermediate, advanced).\n",
      " 3. Choose a learning method (language learning app, textbook, tutor, self-study materials).\n",
      " 4. Set up a schedule for regular practice sessions.\n",
      " 5. Enroll in a French course or find a language exchange partner.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research and find reliable French language learning resources online.', 'Determine the level of proficiency desired (beginner, intermediate, advanced).', 'Choose a learning method (language learning app, textbook, tutor, self-study materials).', 'Set up a schedule for regular practice sessions.', \n",
      "Subtasks:\n",
      " 1. Research French language learning resources online.\n",
      " 2. Download or subscribe to a French language learning app.\n",
      " 3. Find a local French language exchange partner or tutor.\n",
      " 4. Start with basic French grammar rules and vocabulary.\n",
      " 5. Practice speaking, listening, reading, and writing in French daily.\n",
      "\n",
      "=== FORMATTED FINAL SUBTASKS ===\n",
      "1. Research French language learning resources online.\n",
      "2. Download or subscribe to a French language learning app.\n",
      "3. Find a local French language exchange partner or tutor.\n",
      "4. Start with basic French grammar rules and vocabulary.\n",
      "5. Practice speaking, listening, reading, and writing in French daily.\n",
      "Subtasks:\n",
      " 1. Identify the topic or subject of the user's request.\n",
      " 2. Determine if the user is asking for information, assistance with a task, or something else.\n",
      " 3. Clarify any unclear or missing information in the user's request.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=[\"Identify the topic or subject of the user's request.\", 'Determine if the user is asking for information, assistance with a task, or something else.', \"Clarify any unclear or missing information in the user's request.\"]\n",
      "Subtasks:\n",
      " 1. Identify the topic or subject of the user's request.\n",
      " 2. Clarify the nature of the query (e.g., question, problem, task).\n",
      " 3. Extract relevant keywords or phrases from the query.\n",
      " 4. Determine the desired outcome or action.\n",
      " 5. Please provide the actual query for me to work on it\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=[\"Identify the topic or subject of the user's request.\", 'Clarify the nature of the query (e.g., question, problem, task).', 'Extract relevant keywords or phrases from the query.', 'Determine the desired outcome or action.', 'Please provide the actual query for me to work on it']\n",
      "Subtasks:\n",
      " 1. Verify if the user has provided any input.\n",
      " 2. Extract the relevant content from the input if it exists.\n",
      " 3. Analyze the extracted content for a clear request or question.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Verify if the user has provided any input.', 'Extract the relevant content from the input if it exists.', 'Analyze the extracted content for a clear request or question.']\n",
      "Subtasks:\n",
      " 1. Identify the topic or subject of the user's request.\n",
      " 2. Determine if the user is asking for information, guidance, or a solution to a problem.\n",
      " 3. Extract key keywords from the user's request (if provided).\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=[\"Identify the topic or subject of the user's request.\", 'Determine if the user is asking for information, guidance, or a solution to a problem.', \"Extract key keywords from the user's request (if provided).\"]\n",
      "Subtasks:\n",
      " 1. Identify the topic or subject of the user's request.\n",
      " 2. Determine the type of assistance the user is seeking (e.g., information, explanation, solution to a problem).\n",
      " 3. Clarify any unclear or ambiguous terms in the user's request.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=[\"Identify the topic or subject of the user's request.\", 'Determine the type of assistance the user is seeking (e.g., information, explanation, solution to a problem).', \"Clarify any unclear or ambiguous terms in the user's request.\"]\n",
      "Subtasks:\n",
      " 1. Research effective methods for daily vocabulary expansion.\n",
      " 2. Set realistic goals for daily vocabulary acquisition.\n",
      " 3. Identify a suitable time slot for daily vocabulary practice.\n",
      " 4. Choose a vocabulary building tool or resource (e.g., apps, flashcards, books).\n",
      " 5. Create an inventory of words to focus on each day.\n",
      " 6. Develop a routine for reviewing and practicing new words.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research effective methods for daily vocabulary expansion.', 'Set realistic goals for daily vocabulary acquisition.', 'Identify a suitable time slot for daily vocabulary practice.', 'Choose a vocabulary building tool or resource (e.g., apps, flashcards, books).', 'Create an inventory of words to f\n",
      "Subtasks:\n",
      " 1. Research French grammar rules online.\n",
      " 2. Identify common grammar mistakes made by beginners in French language.\n",
      " 3. Choose 5-10 specific grammar rules to focus on for practice.\n",
      " 4. Find online resources (e.g., videos, articles, quizzes) for practicing each rule.\n",
      " 5. Create a schedule to dedicate time per week for practicing French grammar rules.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research French grammar rules online.', 'Identify common grammar mistakes made by beginners in French language.', 'Choose 5-10 specific grammar rules to focus on for practice.', 'Find online resources (e.g., videos, articles, quizzes) for practicing each rule.', 'Create a schedule to dedicate time\n",
      "Subtasks:\n",
      " 1. Retrieve all accepted articles from the database.\n",
      " 2. Filter out rejected or pending articles from the results.\n",
      " 3. Return only the titles and authors of the accepted articles.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Retrieve all accepted articles from the database.', 'Filter out rejected or pending articles from the results.', 'Return only the titles and authors of the accepted articles.']\n",
      "Subtasks:\n",
      " 1. Identify the context of \"forwarding inference\".\n",
      " 2. Understand the definition and scope of \"sufficient\" in relation to forwarding inference.\n",
      " 3. Determine what subtasks are implied by \"forwarding inference\".\n",
      " 4. Break down each implied subtask into its constituent steps.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Identify the context of \"forwarding inference\".', 'Understand the definition and scope of \"sufficient\" in relation to forwarding inference.', 'Determine what subtasks are implied by \"forwarding inference\".', 'Break down each implied subtask into its constituent steps.']\n",
      "Subtasks:\n",
      " 1. Research and gather resources on common French grammar rules.\n",
      " 2. Prioritize a list of key grammar rules to focus on each day.\n",
      " 3. Create flashcards or concept maps to visualize and organize the selected rules.\n",
      " 4. Set aside dedicated time for daily vocabulary expansion (e.g., 30 minutes).\n",
      " 5. Develop a schedule for practicing identified grammar rules, allocating specific days or weeks for review.\n",
      " 6. Identify online resources or language learning apps that offer French grammar lessons and reviews.\n",
      " 7. Create a daily routine to practice grammar rules, including writing exercises, quizzes, or speaking practice.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Research and gather resources on common French grammar rules.', 'Prioritize a list of key grammar rules to focus on each day.', 'Create flashcards or concept maps to visualize and organize the selected rules.', 'Set aside dedicated time for daily vocabulary expansion (e.g., 30 minutes).', 'Develop\n",
      "\n",
      "Top nodes by combined_score:\n",
      "- id=a1562cf2 score=0.8426325259283999 role=final_subtask text=\n",
      "- id=f002bb91 score=0.8426325259283999 role=final_subtask text=\n",
      "- id=ebf90c86 score=0.8398204081783717 role=final_subtask text=\n",
      "- id=0d4e61dc score=0.8398204081783717 role=final_subtask text=\n",
      "- id=3f009444 score=0.839277524498346 role=final_subtask text=\n",
      "- id=9d478f4d score=0.839277524498346 role=final_subtask text=\n",
      "\n",
      "Best path (by summed combined_score): 1.5751760346083274\n",
      " * [root] how can i learn french? (score=None)\n",
      " * [field:message] Subtasks sufficient. Forwarding inference. (score=0.7325435086799276)\n",
      " * [final_subtask]  (score=0.8426325259283999)\n",
      "\n",
      "Saved tree to tree_of_thoughts_example.json\n",
      "\n",
      "=== REASONING DIAGNOSTIC ===\n",
      "{\n",
      "  \"reasoning_score\": 0.7325435086799276,\n",
      "  \"components\": {\n",
      "    \"goal_alignment\": 0.8676738142967224,\n",
      "    \"stepwise_coherence\": 0.6461787819862366,\n",
      "    \"coverage\": 0.6666666666666667,\n",
      "    \"granularity\": 0.44030002227684994,\n",
      "    \"redundancy\": 1.0\n",
      "  },\n",
      "  \"diagnostics\": {\n",
      "    \"stepwise_similarities\": [\n",
      "      0.6293515264987946,\n",
      "      0.6904881596565247,\n",
      "      0.6682824194431305,\n",
      "      0.5965930223464966\n",
      "    ],\n",
      "    \"missing_concepts\": [\n",
      "      \"What are some reliable French language learning resources online?\",\n",
      "      \"How to enroll in a French course or find a language exchange partner\"\n",
      "    ],\n",
      "    \"expected_concepts\": 6,\n",
      "    \"average_step_length\": 9.6,\n",
      "    \"unique_clusters\": 1.0,\n",
      "    \"num_steps\": 5,\n",
      "    \"verifier_notes\": \"Consider starting with beginner-level resources and adjusting as needed\"\n",
      "  },\n",
      "  \"verdict\": \"Reasoning is partially coherent; some gaps or inconsistencies.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# Builds on your existing pipeline, ensures `final_subtasks` are produced by running the decomposer when needed,\n",
    "# normalizes mixed types (str/dict/list), prints formatted subtasks, builds a TreeOfThoughts, attaches reasoning scores,\n",
    "# expands children once, and saves the tree.\n",
    "#\n",
    "# Requirements:\n",
    "# - tree_of_thoughts.py (exists in same folder)\n",
    "# - inference.run_inference_pipeline(prompt, auto_extend=...) available\n",
    "# - inference.run_decomposer(prompt) available (your debug logs show this function)\n",
    "# - CAMRE_EDU.combined_reasoning_score available\n",
    "# - No try/except per user request\n",
    "\n",
    "from tree_of_thoughts import TreeOfThoughts\n",
    "from inference import run_inference_pipeline, run_decomposer\n",
    "from CAMRE_EDU import combined_reasoning_score\n",
    "import json\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_get_text(item):\n",
    "    \"\"\"Return readable text from str/dict/list/tuple or simple fallback str(item).\"\"\"\n",
    "    if item is None:\n",
    "        return \"\"\n",
    "    if isinstance(item, str):\n",
    "        return item.strip()\n",
    "    if isinstance(item, (list, tuple)):\n",
    "        parts = [safe_get_text(x) for x in item]\n",
    "        return \" \".join([p for p in parts if p])\n",
    "    if isinstance(item, dict):\n",
    "        # prefer common keys\n",
    "        for k in (\"text\", \"input\", \"output\", \"content\", \"message\", \"prompt\", \"title\"):\n",
    "            if k in item and item[k] is not None:\n",
    "                return safe_get_text(item[k])\n",
    "        parts = []\n",
    "        for v in item.values():\n",
    "            if isinstance(v, (str, list, tuple, dict)):\n",
    "                parts.append(safe_get_text(v))\n",
    "        joined = \" \".join([p for p in parts if p])\n",
    "        return joined\n",
    "    return str(item)\n",
    "\n",
    "def numeric_scores_from_reasoning(reasoning):\n",
    "    \"\"\"Derive a single numeric combined score from a reasoning dict / numeric value.\"\"\"\n",
    "    if reasoning is None:\n",
    "        return None\n",
    "    if isinstance(reasoning, (int, float)):\n",
    "        return float(reasoning)\n",
    "    if isinstance(reasoning, dict):\n",
    "        for key in (\"reasoning_score\", \"combined\", \"combined_score\", \"confidence\", \"score\", \"overall\"):\n",
    "            if key in reasoning and isinstance(reasoning[key], (int, float)):\n",
    "                return float(reasoning[key])\n",
    "    return None\n",
    "\n",
    "def ensure_numeric_scores(d):\n",
    "    \"\"\"Return numeric-only scores from dict d, else {}.\"\"\"\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {k: v for k, v in d.items() if isinstance(v, (int, float))}\n",
    "\n",
    "def pipeline_to_candidates(pipe_out):\n",
    "    \"\"\"\n",
    "    Normalize pipeline output into a list of canonical candidate dicts:\n",
    "      {\"text\",\"role\",\"scores\",\"metadata\"}\n",
    "    Accepts mixed types (str/dict/list).\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    if pipe_out is None:\n",
    "        return candidates\n",
    "\n",
    "    if not isinstance(pipe_out, dict):\n",
    "        if isinstance(pipe_out, str):\n",
    "            candidates.append({\"text\": pipe_out, \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "        elif isinstance(pipe_out, (list, tuple)):\n",
    "            for it in pipe_out:\n",
    "                candidates.append({\"text\": safe_get_text(it), \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": it}})\n",
    "        return candidates\n",
    "\n",
    "    def ingest(field_value, role_hint):\n",
    "        if field_value is None:\n",
    "            return\n",
    "        if isinstance(field_value, (list, tuple)):\n",
    "            for item in field_value:\n",
    "                text = safe_get_text(item)\n",
    "                meta = item if isinstance(item, dict) else {\"orig\": item}\n",
    "                scores = ensure_numeric_scores(item.get(\"scores\") if isinstance(item, dict) else {})\n",
    "                candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "        else:\n",
    "            text = safe_get_text(field_value)\n",
    "            meta = field_value if isinstance(field_value, dict) else {\"orig\": field_value}\n",
    "            scores = ensure_numeric_scores(field_value.get(\"scores\") if isinstance(field_value, dict) else {})\n",
    "            candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "\n",
    "    ingest(pipe_out.get(\"final_subtasks\"), \"final_subtask\")\n",
    "    ingest(pipe_out.get(\"suggestions\"), \"suggestion\")\n",
    "    ingest(pipe_out.get(\"outputs\"), \"output\")\n",
    "    ingest(pipe_out.get(\"thoughts\"), \"thought\")\n",
    "    ingest(pipe_out.get(\"intermediate_steps\"), \"thought\")\n",
    "\n",
    "    known = {\"final_subtasks\", \"suggestions\", \"outputs\", \"thoughts\", \"intermediate_steps\"}\n",
    "    for k, v in pipe_out.items():\n",
    "        if k in known:\n",
    "            continue\n",
    "        if isinstance(v, (str, list, tuple, dict)):\n",
    "            ingest(v, role_hint=f\"field:{k}\")\n",
    "\n",
    "    if not candidates:\n",
    "        fallback = safe_get_text(pipe_out.get(\"text\") or pipe_out.get(\"content\") or pipe_out)\n",
    "        if fallback:\n",
    "            candidates.append({\"text\": fallback, \"role\": \"fallback\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def ensure_final_subtasks(out, prompt):\n",
    "    \"\"\"\n",
    "    Guarantee that we have a list of final_subtasks as strings/dicts.\n",
    "    If out['final_subtasks'] is missing or contains plain strings that are not parsed,\n",
    "    run the decomposer to produce subtasks.\n",
    "    Returns a list (possibly empty) where each item is either a dict or string.\n",
    "    \"\"\"\n",
    "    if isinstance(out, dict) and \"final_subtasks\" in out:\n",
    "        fs = out[\"final_subtasks\"]\n",
    "        # if it's a non-empty list with useful text, return as-is\n",
    "        if isinstance(fs, (list, tuple)) and any(safe_get_text(x) for x in fs):\n",
    "            return fs\n",
    "        if isinstance(fs, str) and fs.strip():\n",
    "            return [fs]\n",
    "    # Fallback: run the decomposer explicitly to get subtasks\n",
    "    decomposed = run_decomposer(prompt)\n",
    "    # run_decomposer might return a list of strings or a dict with \"subtasks\" etc.\n",
    "    if isinstance(decomposed, dict):\n",
    "        # common shapes: {\"subtasks\": [...]} or {\"final_subtasks\": [...]} or similar\n",
    "        for key in (\"final_subtasks\", \"subtasks\", \"steps\", \"decomposition\"):\n",
    "            if key in decomposed and isinstance(decomposed[key], (list, tuple)):\n",
    "                return decomposed[key]\n",
    "        # if dict itself represents a single subtask\n",
    "        return [decomposed]\n",
    "    if isinstance(decomposed, (list, tuple)):\n",
    "        return decomposed\n",
    "    return [decomposed]\n",
    "\n",
    "def expand_fn_using_pipeline(prompt_text, metadata=None):\n",
    "    \"\"\"\n",
    "    Expand a given node by calling run_inference_pipeline(prompt_text, auto_extend=False)\n",
    "    and normalizing the results into candidate dicts with combined_score attached (if available).\n",
    "    \"\"\"\n",
    "    out = run_inference_pipeline(prompt_text, auto_extend=False)\n",
    "    cands = pipeline_to_candidates(out)\n",
    "    reasoning = combined_reasoning_score(out, prompt_text)\n",
    "    combined_num = numeric_scores_from_reasoning(reasoning)\n",
    "    for c in cands:\n",
    "        c.setdefault(\"scores\", {})\n",
    "        if \"combined_score\" not in c[\"scores\"] and combined_num is not None:\n",
    "            c[\"scores\"][\"combined_score\"] = combined_num\n",
    "        if \"combined_score\" not in c[\"scores\"]:\n",
    "            meta_scores = c.get(\"metadata\", {})\n",
    "            if isinstance(meta_scores, dict) and isinstance(meta_scores.get(\"scores\"), dict):\n",
    "                vals = [v for v in meta_scores[\"scores\"].values() if isinstance(v, (int, float))]\n",
    "                if vals:\n",
    "                    c[\"scores\"][\"combined_score\"] = float(sum(vals) / len(vals))\n",
    "        c[\"scores\"].setdefault(\"combined_score\", 0.0)\n",
    "        c.setdefault(\"metadata\", {})\n",
    "        c[\"metadata\"][\"reasoning_snapshot\"] = reasoning\n",
    "    return cands\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    example_prompt = \"how can i learn french?\"\n",
    "\n",
    "    # 1) Run pipeline with auto_extend True to let it produce its outputs\n",
    "    out = run_inference_pipeline(example_prompt, auto_extend=True)\n",
    "\n",
    "    # 2) Ensure we have well-formed final_subtasks (use decomposer if necessary)\n",
    "    normalized_final_subtasks = ensure_final_subtasks(out, example_prompt)\n",
    "\n",
    "    # 3) Print FORMATTED FINAL SUBTASKS (robust)\n",
    "    print(\"\\n=== FORMATTED FINAL SUBTASKS ===\")\n",
    "    if isinstance(normalized_final_subtasks, (list, tuple)):\n",
    "        for i, s in enumerate(normalized_final_subtasks, start=1):\n",
    "            print(f\"{i}. {safe_get_text(s)}\")\n",
    "    else:\n",
    "        print(\"1.\", safe_get_text(normalized_final_subtasks))\n",
    "\n",
    "    # 4) Build a TreeOfThoughts with the prompt as root\n",
    "    tree = TreeOfThoughts()\n",
    "    root = tree.add_node(text=example_prompt, parent_id=None, role=\"root\", metadata={\"source\": \"user_prompt\"})\n",
    "    tree.set_root(root.id)\n",
    "\n",
    "    # 5) Convert pipeline output into candidates and attach to root\n",
    "    initial_candidates = pipeline_to_candidates(out)\n",
    "    # If pipeline didn't produce candidates, create from normalized_final_subtasks\n",
    "    if not initial_candidates:\n",
    "        for s in normalized_final_subtasks:\n",
    "            initial_candidates.append({\"text\": safe_get_text(s), \"role\": \"final_subtask\", \"scores\": {}, \"metadata\": {\"orig\": s}})\n",
    "\n",
    "    # 6) Compute reasoning diagnostic and attach numeric combined score to nodes\n",
    "    reasoning_result = combined_reasoning_score(out, example_prompt)\n",
    "    root_combined = numeric_scores_from_reasoning(reasoning_result)\n",
    "    tree.nodes[root.id].metadata[\"reasoning_diagnostic\"] = reasoning_result\n",
    "\n",
    "    for cand in initial_candidates:\n",
    "        scores = cand.get(\"scores\", {})\n",
    "        if \"combined_score\" not in scores:\n",
    "            scores[\"combined_score\"] = root_combined if root_combined is not None else 0.0\n",
    "        tree.add_node(\n",
    "            text=cand.get(\"text\", \"\"),\n",
    "            parent_id=root.id,\n",
    "            role=cand.get(\"role\", \"final_subtask\"),\n",
    "            scores=scores,\n",
    "            metadata=cand.get(\"metadata\", {})\n",
    "        )\n",
    "\n",
    "    # 7) Expand each top-level child once using decomposer-aware expansion (beam)\n",
    "    beam_size = 2\n",
    "    for child_id in list(tree.nodes[root.id].children):\n",
    "        # Use expand_fn that calls pipeline; expand_with will take top `beam` returned candidates\n",
    "        tree.expand_with(child_id, expand_fn=expand_fn_using_pipeline, beam=beam_size)\n",
    "\n",
    "    # 8) Print top nodes by combined_score\n",
    "    top_nodes = tree.top_k_by_score(\"combined_score\", k=6)\n",
    "    print(\"\\nTop nodes by combined_score:\")\n",
    "    for n in top_nodes:\n",
    "        print(f\"- id={n.id[:8]} score={n.scores.get('combined_score')} role={n.role} text={n.text[:240]}\")\n",
    "\n",
    "    # 9) Print best path by summed combined_score\n",
    "    path_nodes, path_score = tree.best_path(\"combined_score\", max_depth=6)\n",
    "    print(\"\\nBest path (by summed combined_score):\", path_score)\n",
    "    for n in path_nodes:\n",
    "        print(f\" * [{n.role}] {n.text[:240]} (score={n.scores.get('combined_score')})\")\n",
    "\n",
    "    # 10) Save tree\n",
    "    outpath = \"tree_of_thoughts_example.json\"\n",
    "    tree.save_json(outpath)\n",
    "    print(f\"\\nSaved tree to {outpath}\")\n",
    "\n",
    "    # 11) Print full reasoning diagnostic JSON\n",
    "    print(\"\\n=== REASONING DIAGNOSTIC ===\")\n",
    "    print(json.dumps(reasoning_result, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a285a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3211a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanc\\anaconda3\\envs\\ceras\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\amanc\\Desktop\\ceras\\src\\ceras\\llm_utils.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Embedding model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanc\\Desktop\\ceras\\src\\ceras\\llm_utils.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = llm(prompt)                 # callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FORMATTED FINAL SUBTASKS ===\n",
      "1. Look up reputable online resources for learning French.\n",
      "2. Find a language learning app or website that offers French lessons.\n",
      "3. Consult with a language exchange partner or tutor who speaks French.\n",
      "4. Purchase a textbook or audio materials to supplement learning.\n",
      "5. Set aside time each week to practice speaking and listening in French.\n",
      "Warning: verifier did not return clean JSON. Falling back to heuristic coverage. Error: substring not found\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "from tree_of_thoughts import TreeOfThoughts\n",
    "from inference import run_inference_pipeline, run_decomposer\n",
    "from CAMRE_EDU import combined_reasoning_score\n",
    "import json\n",
    "import io\n",
    "import contextlib\n",
    "from typing import Any, List\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def safe_get_text(item: Any) -> str:\n",
    "    \"\"\"Return readable text from str/dict/list/tuple; collapse whitespace.\"\"\"\n",
    "    if item is None:\n",
    "        return \"\"\n",
    "    if isinstance(item, str):\n",
    "        return \" \".join(item.split())\n",
    "    if isinstance(item, (list, tuple)):\n",
    "        parts = [safe_get_text(x) for x in item]\n",
    "        return \" \".join([p for p in parts if p])\n",
    "    if isinstance(item, dict):\n",
    "        # prefer keys likely to hold user-visible text\n",
    "        for k in (\"text\", \"input\", \"output\", \"content\", \"message\", \"prompt\", \"title\", \"step\"):\n",
    "            if k in item and item[k] is not None:\n",
    "                return safe_get_text(item[k])\n",
    "        # else join string-like values\n",
    "        parts = []\n",
    "        for v in item.values():\n",
    "            if isinstance(v, (str, list, tuple, dict)):\n",
    "                parts.append(safe_get_text(v))\n",
    "        return \" \".join([p for p in parts if p])\n",
    "    return str(item)\n",
    "\n",
    "def is_debug_or_control_text(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic to detect and filter out debug/control messages:\n",
    "    - very short strings (< 4 chars)\n",
    "    - strings that look like internal notes: start with 'subtasks', 'debug', 'warning', 'verifier',\n",
    "      or are of the form 'Subtasks sufficient', 'Forwarding inference', etc.\n",
    "    - numeric-only or punctuation-only\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    t = text.strip()\n",
    "    if len(t) == 0:\n",
    "        return True\n",
    "    if len(t) < 4:\n",
    "        return True\n",
    "    tl = t.lower()\n",
    "    debug_prefixes = (\"subtasks\", \"debug\", \"warning\", \"verifier\", \"fallback\", \"note:\", \"error\", \"pipeline\")\n",
    "    for p in debug_prefixes:\n",
    "        if tl.startswith(p):\n",
    "            return True\n",
    "    # typical one-off control sentences we saw\n",
    "    if \"forwarding inference\" in tl or \"subtasks sufficient\" in tl:\n",
    "        return True\n",
    "    # numeric-only\n",
    "    if all(ch.isdigit() or ch.isspace() for ch in tl):\n",
    "        return True\n",
    "    # punctuation-only\n",
    "    if all(not ch.isalnum() for ch in tl):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def numeric_scores_from_reasoning(reasoning: Any):\n",
    "    \"\"\"Derive a single numeric combined score from a reasoning dict or numeric value.\"\"\"\n",
    "    if reasoning is None:\n",
    "        return None\n",
    "    if isinstance(reasoning, (int, float)):\n",
    "        return float(reasoning)\n",
    "    if isinstance(reasoning, dict):\n",
    "        for key in (\"reasoning_score\", \"combined\", \"combined_score\", \"confidence\", \"score\", \"overall\"):\n",
    "            v = reasoning.get(key)\n",
    "            if isinstance(v, (int, float)):\n",
    "                return float(v)\n",
    "    return None\n",
    "\n",
    "def ensure_numeric_scores(d: Any):\n",
    "    if not isinstance(d, dict):\n",
    "        return {}\n",
    "    return {k: v for k, v in d.items() if isinstance(v, (int, float))}\n",
    "\n",
    "def pipeline_to_candidates(pipe_out: Any) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Normalize pipeline output into list of canonical candidate dicts:\n",
    "    {\"text\",\"role\",\"scores\",\"metadata\"}.\n",
    "    Filters out debug/control texts using is_debug_or_control_text.\n",
    "    \"\"\"\n",
    "    candidates: List[dict] = []\n",
    "    if pipe_out is None:\n",
    "        return candidates\n",
    "\n",
    "    if not isinstance(pipe_out, dict):\n",
    "        if isinstance(pipe_out, str):\n",
    "            text = safe_get_text(pipe_out)\n",
    "            if not is_debug_or_control_text(text):\n",
    "                candidates.append({\"text\": text, \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "        elif isinstance(pipe_out, (list, tuple)):\n",
    "            for it in pipe_out:\n",
    "                text = safe_get_text(it)\n",
    "                if not is_debug_or_control_text(text):\n",
    "                    candidates.append({\"text\": text, \"role\": \"raw\", \"scores\": {}, \"metadata\": {\"orig\": it}})\n",
    "        return candidates\n",
    "\n",
    "    # Helper to ingest field while skipping debug-like entries\n",
    "    def ingest(field_value: Any, role_hint: str):\n",
    "        if field_value is None:\n",
    "            return\n",
    "        if isinstance(field_value, (list, tuple)):\n",
    "            for item in field_value:\n",
    "                text = safe_get_text(item)\n",
    "                if is_debug_or_control_text(text):\n",
    "                    continue\n",
    "                meta = item if isinstance(item, dict) else {\"orig\": item}\n",
    "                scores = ensure_numeric_scores(item.get(\"scores\") if isinstance(item, dict) else {})\n",
    "                candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "        else:\n",
    "            text = safe_get_text(field_value)\n",
    "            if is_debug_or_control_text(text):\n",
    "                return\n",
    "            meta = field_value if isinstance(field_value, dict) else {\"orig\": field_value}\n",
    "            scores = ensure_numeric_scores(field_value.get(\"scores\") if isinstance(field_value, dict) else {})\n",
    "            candidates.append({\"text\": text, \"role\": role_hint, \"scores\": scores, \"metadata\": meta})\n",
    "\n",
    "    # ingest canonical fields (prioritized)\n",
    "    ingest(pipe_out.get(\"final_subtasks\"), \"final_subtask\")\n",
    "    ingest(pipe_out.get(\"suggestions\"), \"suggestion\")\n",
    "    ingest(pipe_out.get(\"outputs\"), \"output\")\n",
    "    ingest(pipe_out.get(\"thoughts\"), \"thought\")\n",
    "    ingest(pipe_out.get(\"intermediate_steps\"), \"thought\")\n",
    "\n",
    "    # permissive ingestion of other top-level fields, but skip 'message' if it looks like control text\n",
    "    known = {\"final_subtasks\", \"suggestions\", \"outputs\", \"thoughts\", \"intermediate_steps\"}\n",
    "    for k, v in pipe_out.items():\n",
    "        if k in known:\n",
    "            continue\n",
    "        # skip fields that are likely debug-only by name\n",
    "        if k.lower() in (\"message\", \"debug\", \"warning\", \"verifier\", \"log\"):\n",
    "            # still inspect v, but only add if it looks substantive\n",
    "            text = safe_get_text(v)\n",
    "            if not is_debug_or_control_text(text):\n",
    "                ingest(v, role_hint=f\"field:{k}\")\n",
    "            continue\n",
    "        if isinstance(v, (str, list, tuple, dict)):\n",
    "            ingest(v, role_hint=f\"field:{k}\")\n",
    "\n",
    "    # fallback to top-level text/content only if no candidates were found\n",
    "    if not candidates:\n",
    "        fallback = safe_get_text(pipe_out.get(\"text\") or pipe_out.get(\"content\") or pipe_out)\n",
    "        if fallback and not is_debug_or_control_text(fallback):\n",
    "            candidates.append({\"text\": fallback, \"role\": \"fallback\", \"scores\": {}, \"metadata\": {\"orig\": pipe_out}})\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def ensure_final_subtasks(out: Any, prompt: str) -> (List[Any], str):\n",
    "    \"\"\"\n",
    "    Guarantee a list of final_subtasks. Use run_decomposer when pipeline output doesn't include meaningful final_subtasks.\n",
    "    Returns (final_subtasks_list, decomposer_debug_output).\n",
    "    \"\"\"\n",
    "    if isinstance(out, dict) and \"final_subtasks\" in out:\n",
    "        fs = out[\"final_subtasks\"]\n",
    "        # If fs is list-like and has at least one non-debug text, keep it\n",
    "        if isinstance(fs, (list, tuple)) and any(not is_debug_or_control_text(safe_get_text(x)) for x in fs):\n",
    "            # return only the substantive items (filter debug ones)\n",
    "            filtered = [x for x in fs if not is_debug_or_control_text(safe_get_text(x))]\n",
    "            return filtered, \"\"\n",
    "        # if it's a non-empty string, wrap and return\n",
    "        if isinstance(fs, str) and fs.strip() and not is_debug_or_control_text(fs):\n",
    "            return [fs], \"\"\n",
    "    # fallback to running decomposer (capture its printed debug)\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf):\n",
    "        decomposed = run_decomposer(prompt)\n",
    "    debug = buf.getvalue()\n",
    "    if isinstance(decomposed, dict):\n",
    "        for key in (\"final_subtasks\", \"subtasks\", \"steps\", \"decomposition\"):\n",
    "            if key in decomposed and isinstance(decomposed[key], (list, tuple)):\n",
    "                filtered = [x for x in decomposed[key] if not is_debug_or_control_text(safe_get_text(x))]\n",
    "                return filtered, debug\n",
    "        # dict itself considered a single item unless it's clearly debug\n",
    "        text = safe_get_text(decomposed)\n",
    "        if not is_debug_or_control_text(text):\n",
    "            return [decomposed], debug\n",
    "        return [], debug\n",
    "    if isinstance(decomposed, (list, tuple)):\n",
    "        filtered = [x for x in decomposed if not is_debug_or_control_text(safe_get_text(x))]\n",
    "        return filtered, debug\n",
    "    # single string or other\n",
    "    text = safe_get_text(decomposed)\n",
    "    if text and not is_debug_or_control_text(text):\n",
    "        return [decomposed], debug\n",
    "    return [], debug\n",
    "\n",
    "def expand_fn_using_pipeline(prompt_text: str, metadata=None):\n",
    "    \"\"\"\n",
    "    Expand a node by calling run_inference_pipeline(prompt_text, auto_extend=False).\n",
    "    Return normalized candidates and the captured debug string.\n",
    "    \"\"\"\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf):\n",
    "        out = run_inference_pipeline(prompt_text, auto_extend=False)\n",
    "    debug = buf.getvalue()\n",
    "    cands = pipeline_to_candidates(out)\n",
    "    reasoning = combined_reasoning_score(out, prompt_text)\n",
    "    combined_num = numeric_scores_from_reasoning(reasoning)\n",
    "    for c in cands:\n",
    "        c.setdefault(\"scores\", {})\n",
    "        if \"combined_score\" not in c[\"scores\"] and combined_num is not None:\n",
    "            c[\"scores\"][\"combined_score\"] = combined_num\n",
    "        if \"combined_score\" not in c[\"scores\"]:\n",
    "            meta_scores = c.get(\"metadata\", {})\n",
    "            if isinstance(meta_scores, dict) and isinstance(meta_scores.get(\"scores\"), dict):\n",
    "                vals = [v for v in meta_scores[\"scores\"].values() if isinstance(v, (int, float))]\n",
    "                if vals:\n",
    "                    c[\"scores\"][\"combined_score\"] = float(sum(vals) / len(vals))\n",
    "        c[\"scores\"].setdefault(\"combined_score\", 0.0)\n",
    "        c.setdefault(\"metadata\", {})\n",
    "        c[\"metadata\"][\"reasoning_snapshot\"] = reasoning\n",
    "    return cands, debug\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    prompt = \"how can i learn french?\"\n",
    "\n",
    "    # 1) Run pipeline with auto_extend True; capture printed debug\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf):\n",
    "        out = run_inference_pipeline(prompt, auto_extend=True)\n",
    "    pipeline_debug = buf.getvalue()\n",
    "\n",
    "    # 2) Ensure final_subtasks exist (use decomposer if necessary)\n",
    "    final_subtasks, decomposer_debug = ensure_final_subtasks(out, prompt)\n",
    "\n",
    "    # 3) Print formatted final_subtasks cleanly\n",
    "    print(\"\\n=== FORMATTED FINAL SUBTASKS ===\")\n",
    "    if final_subtasks:\n",
    "        for i, s in enumerate(final_subtasks, start=1):\n",
    "            print(f\"{i}. {safe_get_text(s)}\")\n",
    "    else:\n",
    "        print(\"No substantive final_subtasks produced.\")\n",
    "\n",
    "    # 4) Build the TreeOfThoughts and attach root diagnostics\n",
    "    tree = TreeOfThoughts()\n",
    "    root = tree.add_node(text=prompt, parent_id=None, role=\"root\", metadata={\"source\": \"user_prompt\"})\n",
    "    tree.set_root(root.id)\n",
    "\n",
    "    reasoning_result = combined_reasoning_score(out, prompt)\n",
    "    root_combined = numeric_scores_from_reasoning(reasoning_result)\n",
    "    tree.nodes[root.id].metadata[\"reasoning_diagnostic\"] = reasoning_result\n",
    "\n",
    "    # 5) Build initial candidates from pipeline output, but filter them\n",
    "    initial_candidates = pipeline_to_candidates(out)\n",
    "    # If pipeline didn't emit candidates, use final_subtasks as fallback\n",
    "    if not initial_candidates and final_subtasks:\n",
    "        for s in final_subtasks:\n",
    "            txt = safe_get_text(s)\n",
    "            if not is_debug_or_control_text(txt):\n",
    "                initial_candidates.append({\"text\": txt, \"role\": \"final_subtask\", \"scores\": {}, \"metadata\": {\"orig\": s}})\n",
    "\n",
    "    # 6) Add filtered candidates to tree (skip any with empty/invalid text)\n",
    "    for cand in initial_candidates:\n",
    "        text = safe_get_text(cand.get(\"text\", \"\"))\n",
    "        if is_debug_or_control_text(text):\n",
    "            continue\n",
    "        scores = cand.get(\"scores\", {})\n",
    "        if \"combined_score\" not in scores:\n",
    "            scores[\"combined_score\"] = root_combined if root_combined is not None else 0.0\n",
    "        tree.add_node(text=text, parent_id=root.id, role=cand.get(\"role\", \"final_subtask\"), scores=scores, metadata=cand.get(\"metadata\", {}))\n",
    "\n",
    "    # 7) Expand children once (beam), capturing expansion debug logs\n",
    "    expansion_debugs: List[str] = []\n",
    "    beam = 2\n",
    "    for child_id in list(tree.nodes[root.id].children):\n",
    "        cands, dbg = expand_fn_using_pipeline(tree.nodes[child_id].text, tree.nodes[child_id].metadata)\n",
    "        expansion_debugs.append(dbg)\n",
    "        # filter candidates and sort by combined_score\n",
    "        filtered = [c for c in cands if not is_debug_or_control_text(safe_get_text(c.get(\"text\", \"\")))]\n",
    "        filtered_sorted = sorted(filtered, key=lambda x: x.get(\"scores\", {}).get(\"combined_score\", 0.0), reverse=True)[:beam]\n",
    "        for cand in filtered_sorted:\n",
    "            txt = safe_get_text(cand.get(\"text\", \"\"))\n",
    "            if is_debug_or_control_text(txt):\n",
    "                continue\n",
    "            tree.add_node(text=txt, parent_id=child_id, role=cand.get(\"role\"), scores=cand.get(\"scores\", {}), metadata=cand.get(\"metadata\", {}))\n",
    "\n",
    "    # 8) Print top nodes by combined_score (only substantive nodes)\n",
    "    top_nodes = tree.top_k_by_score(\"combined_score\", k=8)\n",
    "    # filter out nodes with empty or debug text\n",
    "    top_nodes = [n for n in top_nodes if not is_debug_or_control_text(safe_get_text(n.text))]\n",
    "    print(\"\\nTop nodes by combined_score:\")\n",
    "    for n in top_nodes:\n",
    "        print(f\"- id={n.id[:8]} score={n.scores.get('combined_score')} role={n.role} text={n.text}\")\n",
    "\n",
    "    # 9) Best path by summed combined_score (skip debug nodes when evaluating)\n",
    "    path_nodes, path_score = tree.best_path(\"combined_score\", max_depth=6)\n",
    "    # trim debug nodes from path for clearer display\n",
    "    path_nodes_clean = [n for n in path_nodes if not is_debug_or_control_text(safe_get_text(n.text))]\n",
    "    print(\"\\nBest path (by summed combined_score):\", path_score)\n",
    "    for n in path_nodes_clean:\n",
    "        print(f\" * [{n.role}] {n.text} (score={n.scores.get('combined_score')})\")\n",
    "\n",
    "    # 10) Save tree and debug logs\n",
    "    outpath = \"tree_of_thoughts_example.json\"\n",
    "    tree.save_json(outpath)\n",
    "    print(f\"\\nSaved tree to {outpath}\")\n",
    "\n",
    "    # Save debug logs so you can inspect what pipeline/decomposer printed\n",
    "    with open(\"pipeline_debug.log\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== PIPELINE DEBUG ===\\n\\n\")\n",
    "        f.write(pipeline_debug)\n",
    "        f.write(\"\\n\\n=== DECOMPOSER DEBUG ===\\n\\n\")\n",
    "        f.write(decomposer_debug)\n",
    "        f.write(\"\\n\\n=== EXPANSION DEBUG (concatenated) ===\\n\\n\")\n",
    "        f.write(\"\\n---\\n\".join(expansion_debugs))\n",
    "\n",
    "    # 11) Print reasoning diagnostic\n",
    "    print(\"\\n=== REASONING DIAGNOSTIC ===\")\n",
    "    print(json.dumps(reasoning_result, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4096f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tree_id', 'root_id', 'nodes'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "p = \"../../tree_of_thoughts_example.json\"\n",
    "with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "# print top-level keys\n",
    "print(data.keys())\n",
    "# Inspect nodes that have non-empty metadata\n",
    "for nid, n in data[\"nodes\"].items():\n",
    "    meta = n.get(\"metadata\", {})\n",
    "    text = n.get(\"text\", \"\")\n",
    "    if text.strip() == \"\" and meta:\n",
    "        print(\"node\", nid, \"has metadata keys:\", list(meta.keys()))\n",
    "        # print a preview\n",
    "        print(\"  text:\", text)\n",
    "        print(\"  metadata preview:\", {k: str(meta[k])[:200] for k in meta})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2099a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8cbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c268791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanc\\anaconda3\\envs\\ceras\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\amanc\\Desktop\\ceras\\src\\ceras\\llm_utils.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=MODEL)\n"
     ]
    }
   ],
   "source": [
    "# pipeline_tot_simple.py\n",
    "# Bounded Tree-of-Thoughts: 34 LLM calls max\n",
    "\n",
    "from tree_of_thoughts import TreeOfThoughts\n",
    "from inference import run_decomposer, run_inference_pipeline\n",
    "import time, io, contextlib\n",
    "from typing import List\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "MAX_SUBTASKS = 4\n",
    "MAX_EXPANSIONS = 2   # how many subtasks to expand\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def normalize(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if isinstance(x, str):\n",
    "        return \" \".join(x.split())\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return \" \".join(normalize(i) for i in x if normalize(i))\n",
    "    if isinstance(x, dict):\n",
    "        for k in (\"text\", \"content\", \"message\", \"step\"):\n",
    "            if k in x:\n",
    "                return normalize(x[k])\n",
    "    return \"\"\n",
    "\n",
    "def clean(items: List[str]) -> List[str]:\n",
    "    out, seen = [], set()\n",
    "    for s in items:\n",
    "        t = normalize(s)\n",
    "        if not t:\n",
    "            continue\n",
    "        k = t.lower()\n",
    "        if k in seen:\n",
    "            continue\n",
    "        seen.add(k)\n",
    "        out.append(t)\n",
    "    return out[:MAX_SUBTASKS]\n",
    "\n",
    "# ---------------- main ----------------\n",
    "def main(prompt: str):\n",
    "    \"\"\"\n",
    "    Guarantees:\n",
    "    - small ToT\n",
    "    - <= 4 LLM calls\n",
    "    - no recursion\n",
    "    \"\"\"\n",
    "\n",
    "    tree = TreeOfThoughts()\n",
    "\n",
    "    # ---- root ----\n",
    "    root = tree.add_node(\n",
    "        text=prompt,\n",
    "        parent_id=None,\n",
    "        role=\"root\",\n",
    "        metadata={\"ts\": time.time()}\n",
    "    )\n",
    "    tree.set_root(root.id)\n",
    "\n",
    "    # ---- 1) Decompose ONCE ----\n",
    "    buf = io.StringIO()\n",
    "    with contextlib.redirect_stdout(buf):\n",
    "        subtasks = run_decomposer(prompt)\n",
    "\n",
    "    if not isinstance(subtasks, list):\n",
    "        subtasks = [subtasks]\n",
    "\n",
    "    subtasks = clean(subtasks)\n",
    "\n",
    "    # ---- add subtasks as children ----\n",
    "    subtask_nodes = []\n",
    "    for s in subtasks:\n",
    "        n = tree.add_node(\n",
    "            text=s,\n",
    "            parent_id=root.id,\n",
    "            role=\"subtask\",\n",
    "            metadata={\"source\": \"decomposer\"}\n",
    "        )\n",
    "        subtask_nodes.append(n)\n",
    "\n",
    "    # ---- 2) Expand only top-k subtasks ONCE ----\n",
    "    for n in subtask_nodes[:MAX_EXPANSIONS]:\n",
    "        out = run_inference_pipeline(n.text, auto_extend=False)\n",
    "\n",
    "        expansions = []\n",
    "        if isinstance(out, dict):\n",
    "            for key in (\"thoughts\", \"suggestions\", \"steps\", \"outputs\"):\n",
    "                if key in out:\n",
    "                    vals = out[key]\n",
    "                    if not isinstance(vals, list):\n",
    "                        vals = [vals]\n",
    "                    expansions.extend(vals)\n",
    "\n",
    "        expansions = clean(expansions)[:2]  # very small fan-out\n",
    "\n",
    "        for e in expansions:\n",
    "            tree.add_node(\n",
    "                text=e,\n",
    "                parent_id=n.id,\n",
    "                role=\"thought\",\n",
    "                metadata={\"source\": \"inference\"}\n",
    "            )\n",
    "\n",
    "    # ---- done ----\n",
    "    tree.save_json(\"tree_of_thoughts_example.json\")\n",
    "\n",
    "    # ---- print for visibility ----\n",
    "    print(\"\\n=== TREE (SIMPLE ToT) ===\")\n",
    "    print(prompt)\n",
    "    for c in tree.nodes[root.id].children:\n",
    "        print(\" \", tree.nodes[c].text)\n",
    "        for gc in tree.nodes[c].children:\n",
    "            print(\"    \", tree.nodes[gc].text)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # -------------------- SAVE TREE (JSON) --------------------\n",
    "    JSON_PATH = \"tree_of_thoughts_example.json\"\n",
    "    tree.save_json(JSON_PATH)\n",
    "    print(f\"[INFO] Tree saved to {JSON_PATH}\")\n",
    "    \n",
    "    return {\n",
    "        \"tree\": tree,\n",
    "        \"llm_calls_used\": 1 + min(len(subtask_nodes), MAX_EXPANSIONS)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dcc027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7842e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671c4383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amanc\\Desktop\\ceras\\src\\ceras\\llm_utils.py:27: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  out = llm(prompt)                 # callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtasks:\n",
      " 1. Browse online mathematical resources for trigonometry.\n",
      " 2. Search for \"trigonometric identities\" on math websites.\n",
      " 3. Evaluate search results to determine relevance.\n",
      " 4. Categorize identified identities by type (e.g., sum/difference, double angle).\n",
      " 5. Record and organize key identities in a searchable database or note-taking system.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Browse online mathematical resources for trigonometry.', 'Search for \"trigonometric identities\" on math websites.', 'Evaluate search results to determine relevance.', 'Categorize identified identities by type (e.g., sum/difference, double angle).', 'Record and organize key identities in a searchab\n",
      "Subtasks:\n",
      " 1. Calculate the value of 51^2.\n",
      " 2. Calculate the value of 49^2.\n",
      " 3. Subtract 49^2 from 51^2.\n",
      " 4. Identify identities that can be applied to simplify the expression from step 3.\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Calculate the value of 51^2.', 'Calculate the value of 49^2.', 'Subtract 49^2 from 51^2.', 'Identify identities that can be applied to simplify the expression from step 3.']\n",
      "\n",
      "=== TREE (SIMPLE ToT) ===\n",
      "what are trignometric identities and how can we solve (51^2 - 49^2) efficiently using them?\n",
      "  Research and identify key trigonometric identities.\n",
      "  Determine which identity applies to solving (51^2 - 49^2).\n",
      "  Break down the expression into a form suitable for applying the selected identity.\n",
      "  Apply the chosen identity to simplify the expression.\n",
      "[INFO] Tree saved to tree_of_thoughts_example.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tree': <tree_of_thoughts.TreeOfThoughts at 0x26674994b60>,\n",
       " 'llm_calls_used': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "# from pipeline_subtasks_only import main\n",
    "\n",
    "PROMPT = (\n",
    "    \"what are trignometric identities and how can we solve \"\n",
    "    \"(51^2 - 49^2) efficiently using them?\"\n",
    ")\n",
    "\n",
    "main(PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf80c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
