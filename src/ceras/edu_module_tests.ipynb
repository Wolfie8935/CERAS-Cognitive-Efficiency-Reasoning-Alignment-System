{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f765d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import subprocess\n",
    "import shlex\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fe4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Embedding model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Choose a sentence-transformers model available locally or will auto-download.\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # fast and effective; change if you prefer\n",
    "\n",
    "print(\"Loading embedding model:\", EMBEDDING_MODEL_NAME)\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70febd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    # SentenceTransformer returns numpy arrays\n",
    "    emb = embed_model.encode([text], show_progress_bar=False, normalize_embeddings=True)\n",
    "    return np.asarray(emb[0])\n",
    "\n",
    "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    if a is None or b is None:\n",
    "        return 0.0\n",
    "    # embeddings normalized in get_embedding; but compute robustly\n",
    "    return float(np.clip(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12), -1.0, 1.0))\n",
    "\n",
    "def sim_to_01(sim: float) -> float:\n",
    "    # map cosine similarity (-1..1) to 0..1\n",
    "    return max(0.0, min(1.0, (sim + 1.0) / 2.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f99458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_text(step: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract a canonical text representation for a ToT step.\n",
    "    Assumes step dict has 'input' and 'output' fields (based on your inference.py).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if step.get(\"input\"):\n",
    "        parts.append(str(step[\"input\"]).strip())\n",
    "    if step.get(\"output\"):\n",
    "        parts.append(str(step[\"output\"]).strip())\n",
    "    return \" \".join(parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6869c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_goal_alignment(prompt: str, final_subtasks: List[Dict[str, Any]], use_last_only: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between the prompt (goal) and the ToT final answer.\n",
    "    If use_last_only: only consider the last step output, else concatenate all outputs.\n",
    "    Returns normalized score in [0,1].\n",
    "    \"\"\"\n",
    "    if not final_subtasks:\n",
    "        return 0.0\n",
    "    if use_last_only:\n",
    "        final_text = step_text(final_subtasks[-1])\n",
    "    else:\n",
    "        final_text = \" \".join([step_text(s) for s in final_subtasks])\n",
    "    if not final_text:\n",
    "        return 0.0\n",
    "    sim = cosine(get_embedding(final_text), get_embedding(prompt))\n",
    "    return sim_to_01(sim)\n",
    "\n",
    "def compute_stepwise_coherence(final_subtasks: List[Dict[str, Any]]) -> float:\n",
    "    \"\"\"\n",
    "    Average adjacent-step similarity (normalized to 0..1).\n",
    "    If only one step, returns 1.0 (perfect local coherence).\n",
    "    \"\"\"\n",
    "    texts = [step_text(s) for s in final_subtasks]\n",
    "    embs = [get_embedding(t) for t in texts if t]\n",
    "    if len(embs) <= 1:\n",
    "        return 1.0\n",
    "    sims = []\n",
    "    for i in range(len(embs) - 1):\n",
    "        sims.append(sim_to_01(cosine(embs[i], embs[i+1])))\n",
    "    return float(sum(sims) / len(sims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bffdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_verifier_coverage(prompt: str, final_subtasks: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses ollama LLM to ask for missing concepts / expected concept count.\n",
    "    The LLM is instructed to output strict JSON:\n",
    "    {\n",
    "      \"missing\": [\"concept1\", \"concept2\", ...],\n",
    "      \"expected_concepts\": 4,\n",
    "      \"notes\": \"short text\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Build a compact representation of steps\n",
    "    steps_text = \"\\n\".join([f\"Step {i+1}: {step_text(s)}\" for i, s in enumerate(final_subtasks)])\n",
    "    verifier_prompt = f\"\"\"\n",
    "You are a verifier assistant. Given the original prompt and the chain-of-thought steps, output a strict JSON object with fields:\n",
    " - missing: a list of missing sub-concepts or checks that should be present for a complete solution (may be empty list).\n",
    " - expected_concepts: integer estimate for the number of major sub-concepts that should be covered.\n",
    " - notes: one-line diagnostic advice.\n",
    "\n",
    "Respond ONLY with JSON (no extra commentary).\n",
    "\n",
    "Original prompt:\n",
    "\\\"\\\"\\\"{prompt}\\\"\\\"\\\"\n",
    "\n",
    "Chain-of-thought steps:\n",
    "\\\"\\\"\\\"{steps_text}\\\"\\\"\\\"\n",
    "\n",
    "Produce the JSON now.\n",
    "\"\"\"\n",
    "    llm = Ollama(model='llama3.2')\n",
    "    raw = llm(verifier_prompt)\n",
    "    # Try to extract JSON substring (some LLMs may wrap). We'll attempt to find the first '{' and last '}'.\n",
    "    try:\n",
    "        start = raw.index(\"{\")\n",
    "        end = raw.rindex(\"}\") + 1\n",
    "        json_text = raw[start:end]\n",
    "        parsed = json.loads(json_text)\n",
    "        # sanitise fields\n",
    "        parsed.setdefault(\"missing\", [])\n",
    "        parsed.setdefault(\"expected_concepts\", max(1, len(final_subtasks)))\n",
    "        parsed.setdefault(\"notes\", \"\")\n",
    "        return parsed\n",
    "    except Exception as e:\n",
    "        # Fallback heuristic: no LLM JSON available, return heuristics\n",
    "        print(\"Warning: verifier did not return clean JSON. Falling back to heuristic coverage. Error:\", e)\n",
    "        # Simple heuristic: expected = len(steps), missing = []\n",
    "        return {\"missing\": [], \"expected_concepts\": max(1, len(final_subtasks)), \"notes\": \"fallback heuristic used\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c75be9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_granularity(final_subtasks: List[Dict[str, Any]], ideal_len: int = 25, sigma: float = 12.0) -> float:\n",
    "    \"\"\"\n",
    "    Score each step by how close its token count (word count) is to ideal_len using a Gaussian mapping.\n",
    "    Returns average score in [0,1].\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    for s in final_subtasks:\n",
    "        l = len(step_text(s).split())\n",
    "        lengths.append(l)\n",
    "    if not lengths:\n",
    "        return 1.0\n",
    "    scores = [math.exp(-((l - ideal_len) ** 2) / (2 * (sigma ** 2))) for l in lengths]\n",
    "    # normalize scores to [0,1] (they already are because exp -> (0,1])\n",
    "    return float(sum(scores) / len(scores))\n",
    "\n",
    "def compute_redundancy(final_subtasks: List[Dict[str, Any]], cluster_threshold: float = 0.85) -> float:\n",
    "    \"\"\"\n",
    "    Greedy clustering by cosine similarity threshold. Returns \"uniqueness\" fraction (higher = less redundant).\n",
    "    \"\"\"\n",
    "    texts = [step_text(s) for s in final_subtasks]\n",
    "    if not texts:\n",
    "        return 1.0\n",
    "    embs = np.vstack([get_embedding(t) for t in texts])\n",
    "    # Use AgglomerativeClustering with distance threshold equivalent to (1 - cluster_threshold)\n",
    "    try:\n",
    "        # sklearn AgglomerativeClustering with distance_threshold requires n_clusters=None\n",
    "        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.0 - cluster_threshold, linkage=\"average\").fit(embs)\n",
    "        labels = clustering.labels_\n",
    "        num_clusters = len(set(labels))\n",
    "    except Exception:\n",
    "        # fallback: greedy grouping\n",
    "        centroids = []\n",
    "        labels = []\n",
    "        for e in embs:\n",
    "            placed = False\n",
    "            for i, c in enumerate(centroids):\n",
    "                if cosine(e, c) >= cluster_threshold:\n",
    "                    # update centroid (mean)\n",
    "                    centroids[i] = (centroids[i] * labels.count(i) + e) / (labels.count(i) + 1)\n",
    "                    labels.append(i)\n",
    "                    placed = True\n",
    "                    break\n",
    "            if not placed:\n",
    "                centroids.append(e)\n",
    "                labels.append(len(centroids) - 1)\n",
    "        num_clusters = len(centroids)\n",
    "    unique_frac = num_clusters / max(1, len(texts))\n",
    "    # uniqueness score: closer to 1 is better (less redundancy)\n",
    "    return float(unique_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90236288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_reasoning_score(out: Dict[str, Any], prompt: str, weights: Optional[Dict[str, float]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute component scores and combined reasoning score.\n",
    "    out: dictionary returned by run_inference_pipeline (expects out['final_subtasks'] list)\n",
    "    prompt: original prompt text\n",
    "    \"\"\"\n",
    "    final_subtasks = out.get(\"final_subtasks\", [])\n",
    "    # components\n",
    "    alignment = compute_goal_alignment(prompt, final_subtasks, use_last_only=True)\n",
    "    coherence = compute_stepwise_coherence(final_subtasks)\n",
    "    granularity = compute_granularity(final_subtasks)\n",
    "    redundancy = compute_redundancy(final_subtasks)\n",
    "    # coverage uses LLM verifier\n",
    "    verifier_info = call_verifier_coverage(prompt, final_subtasks)\n",
    "    missing = verifier_info.get(\"missing\", [])\n",
    "    expected = max(1, int(verifier_info.get(\"expected_concepts\", max(1, len(final_subtasks)))))\n",
    "    coverage = max(0.0, 1.0 - (len(missing) / expected))\n",
    "    # default weights (sum to 1)\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"alignment\": 0.30,\n",
    "            \"coherence\": 0.25,\n",
    "            \"coverage\": 0.25,\n",
    "            \"granularity\": 0.10,\n",
    "            \"redundancy\": 0.10\n",
    "        }\n",
    "    # ensure normalization\n",
    "    s = sum(weights.values())\n",
    "    if s == 0:\n",
    "        raise ValueError(\"weights must sum to > 0\")\n",
    "    for k in weights:\n",
    "        weights[k] = weights[k] / s\n",
    "    reasoning_score = (\n",
    "        weights[\"alignment\"] * alignment\n",
    "        + weights[\"coherence\"] * coherence\n",
    "        + weights[\"coverage\"] * coverage\n",
    "        + weights[\"granularity\"] * granularity\n",
    "        + weights[\"redundancy\"] * redundancy\n",
    "    )\n",
    "    # diagnostics\n",
    "    diagnostics = {\n",
    "        \"stepwise_similarities\": None,  # compute adjacent sims for debugging\n",
    "        \"missing_concepts\": missing,\n",
    "        \"expected_concepts\": expected,\n",
    "        \"average_step_length\": None,\n",
    "        \"unique_clusters\": None,\n",
    "        \"num_steps\": len(final_subtasks),\n",
    "        \"verifier_notes\": verifier_info.get(\"notes\", \"\")\n",
    "    }\n",
    "    # populate additional diagnostics\n",
    "    # adjacent sims\n",
    "    texts = [step_text(s) for s in final_subtasks]\n",
    "    embs = [get_embedding(t) for t in texts if t]\n",
    "    adj_sims = [sim_to_01(cosine(embs[i], embs[i+1])) for i in range(len(embs)-1)] if len(embs) > 1 else []\n",
    "    diagnostics[\"stepwise_similarities\"] = adj_sims\n",
    "    diagnostics[\"average_step_length\"] = float(sum(len(t.split()) for t in texts) / max(1, len(texts)))\n",
    "    # clusters uniqueness\n",
    "    diagnostics[\"unique_clusters\"] = float(compute_redundancy(final_subtasks))\n",
    "    # verdict\n",
    "    if reasoning_score >= 0.75:\n",
    "        verdict = \"Reasoning is coherent and well-aligned with goal.\"\n",
    "    elif reasoning_score >= 0.5:\n",
    "        verdict = \"Reasoning is partially coherent; some gaps or inconsistencies.\"\n",
    "    else:\n",
    "        verdict = \"Reasoning shows substantial drift or incompleteness.\"\n",
    "    return {\n",
    "        \"reasoning_score\": float(reasoning_score),\n",
    "        \"components\": {\n",
    "            \"goal_alignment\": float(alignment),\n",
    "            \"stepwise_coherence\": float(coherence),\n",
    "            \"coverage\": float(coverage),\n",
    "            \"granularity\": float(granularity),\n",
    "            \"redundancy\": float(redundancy)\n",
    "        },\n",
    "        \"diagnostics\": diagnostics,\n",
    "        \"verdict\": verdict\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1afc394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running semantic scorer on example output...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amanc\\AppData\\Local\\Temp\\ipykernel_13992\\2904521226.py:29: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model='llama3.2')\n",
      "C:\\Users\\amanc\\AppData\\Local\\Temp\\ipykernel_13992\\2904521226.py:30: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  raw = llm(verifier_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning_score\": 0.6414356923762288,\n",
      "  \"components\": {\n",
      "    \"goal_alignment\": 0.7514747381210327,\n",
      "    \"stepwise_coherence\": 0.7130378832419714,\n",
      "    \"coverage\": 0.33333333333333337,\n",
      "    \"granularity\": 0.5440046679609276,\n",
      "    \"redundancy\": 1.0\n",
      "  },\n",
      "  \"diagnostics\": {\n",
      "    \"stepwise_similarities\": [\n",
      "      0.6731556951999664,\n",
      "      0.631446972489357,\n",
      "      0.8345109820365906\n",
      "    ],\n",
      "    \"missing_concepts\": [\n",
      "      \"Handle non-numeric values\",\n",
      "      \"Edge case handling for empty list\"\n",
      "    ],\n",
      "    \"expected_concepts\": 3,\n",
      "    \"average_step_length\": 11.75,\n",
      "    \"unique_clusters\": 1.0,\n",
      "    \"num_steps\": 4,\n",
      "    \"verifier_notes\": \"Consider using try/except blocks to catch non-numeric value errors and handle edge cases correctly.\"\n",
      "  },\n",
      "  \"verdict\": \"Reasoning is partially coherent; some gaps or inconsistencies.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example synthetic `out` structure (adapt to your actual run_inference_pipeline output)\n",
    "example_out = {\n",
    "    \"final_subtasks\": [\n",
    "        {\"id\": 1, \"input\": \"Understand the problem: find sum of list\", \"output\": \"We need to sum numeric elements; check for non-numeric values.\"},\n",
    "        {\"id\": 2, \"input\": \"\", \"output\": \"Filter out non-numeric entries and convert strings to floats.\"},\n",
    "        {\"id\": 3, \"input\": \"\", \"output\": \"Use a running sum; consider edge cases: empty list, None entries.\"},\n",
    "        {\"id\": 4, \"input\": \"\", \"output\": \"Return the sum as float; if empty list return 0.\"}\n",
    "    ],\n",
    "    # optionally other fields your pipeline returns\n",
    "}\n",
    "\n",
    "prompt = \"Write a robust plan to compute the sum of a list of items that may include strings and None.\"\n",
    "print(\"Running semantic scorer on example output...\")\n",
    "result = combined_reasoning_score(example_out, prompt)\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e54405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtasks:\n",
      " 1. Recall the formula for difference of squares: (a-b)(a+b)\n",
      " 2. Identify values of a and b in the given equation\n",
      " 3. Apply the formula to simplify the expression\n",
      " 4. Solve for the resulting expression if necessary\n",
      "[DEBUG] run_decomposer returned type=<class 'list'>, preview=['Recall the formula for difference of squares: (a-b)(a+b)', 'Identify values of a and b in the given equation', 'Apply the formula to simplify the expression', 'Solve for the resulting expression if necessary']\n",
      "\n",
      "=== FORMATTED FINAL SUBTASKS ===\n",
      "1. Recall the formula for difference of squares: (a-b)(a+b)\n",
      "2. Identify values of a and b in the given equation\n",
      "3. Apply the formula to simplify the expression\n",
      "4. Solve for the resulting expression if necessary\n"
     ]
    }
   ],
   "source": [
    "from inference import run_inference_pipeline\n",
    "import json\n",
    "\n",
    "example_prompt = \"how can i solve the question (a^2 - b^2)?\"\n",
    "out = run_inference_pipeline(example_prompt,auto_extend=True)\n",
    "\n",
    "print(\"\\n=== FORMATTED FINAL SUBTASKS ===\")\n",
    "for i, s in enumerate(out[\"final_subtasks\"], start=1):\n",
    "    text = s.get(\"input\") or s.get(\"output\") or \"\"\n",
    "    print(f\"{i}. {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dce6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REASONING DIAGNOSTIC ===\n",
      "{\n",
      "  \"reasoning_score\": 0.7063295120804355,\n",
      "  \"components\": {\n",
      "    \"goal_alignment\": 0.7272500991821289,\n",
      "    \"stepwise_coherence\": 0.7385770032803217,\n",
      "    \"coverage\": 0.6666666666666667,\n",
      "    \"granularity\": 0.3684356483904969,\n",
      "    \"redundancy\": 1.0\n",
      "  },\n",
      "  \"diagnostics\": {\n",
      "    \"stepwise_similarities\": [\n",
      "      0.7189392745494843,\n",
      "      0.6485119313001633,\n",
      "      0.8482798039913177\n",
      "    ],\n",
      "    \"missing_concepts\": [\n",
      "      \"values of a and b\"\n",
      "    ],\n",
      "    \"expected_concepts\": 3,\n",
      "    \"average_step_length\": 8.0,\n",
      "    \"unique_clusters\": 1.0,\n",
      "    \"num_steps\": 4,\n",
      "    \"verifier_notes\": \"Clearly identify a and b in the equation (a^2 - b^2)\"\n",
      "  },\n",
      "  \"verdict\": \"Reasoning is partially coherent; some gaps or inconsistencies.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "reasoning_result = combined_reasoning_score(out, example_prompt)\n",
    "print(\"\\n=== REASONING DIAGNOSTIC ===\")\n",
    "print(json.dumps(reasoning_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39504ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
