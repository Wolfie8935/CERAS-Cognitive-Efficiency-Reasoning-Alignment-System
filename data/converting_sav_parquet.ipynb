{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d149ffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT DIR: /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/Dataset\n",
      "OUTPUT DIR: /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw\n",
      "CHUNK_ROWS: 200000\n",
      "COMPRESSION: snappy\n",
      "Starting conversion (run from terminal, not Jupyter recommended)...\n",
      "\n",
      "Found 4 .sav file(s) in /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/Dataset\n",
      "\n",
      "Converting: CY08MSP_SCH_QQQ.SAV\n",
      "  Created writer for: CY08MSP_SCH_QQQ.parquet\n",
      "  Wrote rows 0 .. 21628  (21629 rows)\n",
      "Parquet writer closed.\n",
      "  Completed CY08MSP_SCH_QQQ.SAV in 2.0s -> /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw/CY08MSP_SCH_QQQ.parquet (size: 5.6 MB)\n",
      "Converting: CY08MSP_STU_COG.SAV\n",
      "  Created writer for: CY08MSP_STU_COG.parquet\n",
      "  Wrote rows 0 .. 199999  (200000 rows)\n",
      "  Wrote rows 200000 .. 399999  (200000 rows)\n",
      "  Wrote rows 400000 .. 599999  (200000 rows)\n",
      "  Wrote rows 600000 .. 613743  (13744 rows)\n",
      "Parquet writer closed.\n",
      "  Completed CY08MSP_STU_COG.SAV in 226.5s -> /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw/CY08MSP_STU_COG.parquet (size: 762.0 MB)\n",
      "Converting: CY08MSP_STU_QQQ.SAV\n",
      "  Created writer for: CY08MSP_STU_QQQ.parquet\n",
      "  Wrote rows 0 .. 199999  (200000 rows)\n",
      "  Wrote rows 200000 .. 399999  (200000 rows)\n",
      "  Wrote rows 400000 .. 599999  (200000 rows)\n",
      "  Wrote rows 600000 .. 613743  (13744 rows)\n",
      "Parquet writer closed.\n",
      "  Completed CY08MSP_STU_QQQ.SAV in 93.7s -> /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw/CY08MSP_STU_QQQ.parquet (size: 712.5 MB)\n",
      "Converting: CY08MSP_STU_TIM.SAV\n",
      "  Created writer for: CY08MSP_STU_TIM.parquet\n",
      "  Wrote rows 0 .. 199999  (200000 rows)\n",
      "  Wrote rows 200000 .. 399999  (200000 rows)\n",
      "  Wrote rows 400000 .. 599999  (200000 rows)\n",
      "  Wrote rows 600000 .. 613743  (13744 rows)\n",
      "Parquet writer closed.\n",
      "  Completed CY08MSP_STU_TIM.SAV in 15.5s -> /Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw/CY08MSP_STU_TIM.parquet (size: 220.0 MB)\n"
     ]
    }
   ],
   "source": [
    "import gc, time, sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import pyreadstat\n",
    "except ImportError:\n",
    "    print(\"Please install pyreadstat: pip install pyreadstat\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "except ImportError:\n",
    "    print(\"Please install pyarrow: pip install pyarrow\")\n",
    "    sys.exit(1)\n",
    "\n",
    "INPUT_DIR = Path(\"/Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/Dataset\")\n",
    "OUT_DIR = Path(\"/Users/rishaan/Desktop/CERAS-Cognitive-Efficiency-Reasoning-Alignment-System/data/raw\")\n",
    "CHUNK_ROWS = 200_000  \n",
    "COMPRESSION = \"snappy\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def convert_one_sav_to_parquet(sav_path: Path, out_parquet_path: Path, chunk_rows: int = CHUNK_ROWS):\n",
    "    offset = 0\n",
    "    writer = None\n",
    "    start = time.time()\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                df, meta = pyreadstat.read_sav(str(sav_path), row_limit=chunk_rows, row_offset=offset)\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERROR] reading chunk at offset {offset} for {sav_path.name}: {type(e).__name__}: {e}\")\n",
    "                raise\n",
    "\n",
    "            nrows = 0 if df is None else df.shape[0]\n",
    "            if nrows == 0:\n",
    "                if offset == 0:\n",
    "                    print(f\"  [WARN] No rows read from {sav_path} (empty or unreadable).\")\n",
    "                break\n",
    "\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(str(out_parquet_path), table.schema, compression=COMPRESSION)\n",
    "                print(f\"  Created writer for: {out_parquet_path.name}\")\n",
    "\n",
    "            writer.write_table(table)\n",
    "            print(f\"  Wrote rows {offset} .. {offset + nrows - 1}  ({nrows} rows)\")\n",
    "\n",
    "            offset += nrows\n",
    "            del df, meta, table\n",
    "            gc.collect()\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[INTERRUPT] User requested stop.\")\n",
    "        raise\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "            print(\"Parquet writer closed.\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  Completed {sav_path.name} in {elapsed:.1f}s -> {out_parquet_path} (size: {out_parquet_path.stat().st_size / 1024**2:.1f} MB)\")\n",
    "\n",
    "def main():\n",
    "    if not INPUT_DIR.exists() or not INPUT_DIR.is_dir():\n",
    "        print(\"ERROR: INPUT_DIR not found or not a directory:\", INPUT_DIR)\n",
    "        sys.exit(1)\n",
    "\n",
    "    sav_files = sorted([p for p in INPUT_DIR.glob(\"*.sav\")] + [p for p in INPUT_DIR.glob(\"*.SAV\")])\n",
    "    if not sav_files:\n",
    "        print(\"No .sav files found in:\", INPUT_DIR)\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(f\"Found {len(sav_files)} .sav file(s) in {INPUT_DIR}\\n\")\n",
    "    for sav in sav_files:\n",
    "        try:\n",
    "            out_parquet = OUT_DIR / (sav.stem + \".parquet\")\n",
    "            if out_parquet.exists():\n",
    "                print(f\"Skipping {sav.name} â€” output already exists: {out_parquet.name}\")\n",
    "                continue\n",
    "            print(f\"Converting: {sav.name}\")\n",
    "            convert_one_sav_to_parquet(sav, out_parquet, chunk_rows=CHUNK_ROWS)\n",
    "        except Exception as exc:\n",
    "            print(f\"[FAILED] {sav.name}: {type(exc).__name__}: {exc}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"INPUT DIR:\", INPUT_DIR)\n",
    "    print(\"OUTPUT DIR:\", OUT_DIR)\n",
    "    print(\"CHUNK_ROWS:\", CHUNK_ROWS)\n",
    "    print(\"COMPRESSION:\", COMPRESSION)\n",
    "    print(\"Starting conversion (run from terminal, not Jupyter recommended)...\\n\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
